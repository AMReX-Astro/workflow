<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Managing Jobs at NERSC &mdash; AMReX-Astro 1.0 documentation</title>
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Visualization at NERSC" href="nersc-visualization.html" />
    <link rel="prev" title="Compiling at NERSC" href="nersc-compilers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> AMReX-Astro
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">AMReX Astro basics</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="nersc.html">Working at NERSC</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="nersc-compilers.html">Compiling at NERSC</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Managing Jobs at NERSC</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cori-haswell">Cori Haswell</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cori-gpu">Cori GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perlmutter">Perlmutter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#archiving-data-to-hpss">Archiving Data to HPSS</a></li>
<li class="toctree-l2"><a class="reference internal" href="nersc-visualization.html">Visualization at NERSC</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="olcf.html">Working at OLCF</a></li>
<li class="toctree-l1"><a class="reference internal" href="iacs.html">Working at IACS</a></li>
<li class="toctree-l1"><a class="reference internal" href="workstations.html">Linux Workstations</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMReX-Astro</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="nersc.html">Working at NERSC</a> &raquo;</li>
      <li>Managing Jobs at NERSC</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/nersc-workflow.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="managing-jobs-at-nersc">
<h1>Managing Jobs at NERSC<a class="headerlink" href="#managing-jobs-at-nersc" title="Permalink to this headline">¶</a></h1>
<section id="cori-haswell">
<h2>Cori Haswell<a class="headerlink" href="#cori-haswell" title="Permalink to this headline">¶</a></h2>
<p>Cori Haswell is configured with 32 cores per node, split between
twi Intel Haswell 16-core processors.</p>
<p>Jobs should be run in your <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory. By default,
SLURM will change directory into the submission directory.</p>
<p>A sample job submission script for pure MPI,
<code class="docutils literal notranslate"><span class="pre">cori_haswell.MPI.slurm</span></code> is in the <a class="reference external" href="https://github.com/AMReX-Astro/workflow/blob/master/job_scripts/cori-haswell/">cori-haswell</a>
directory (<code class="docutils literal notranslate"><span class="pre">workflow/job_scripts/cori-haswell/</span></code>) and includes logic
to automatically add the correct restart options to the run to
continue a simulation from the last checkpoint file in the submission
directory.</p>
<p>Jobs are submitted as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch script.slurm
</pre></div>
</div>
<p>To chain jobs, such that one queues up after the previous job
finished, use the <code class="docutils literal notranslate"><span class="pre">chainslurm.sh</span></code> script in that same directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chainslurm.sh jobid number script
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">jobid</span></code> is the existing job you want to start you chain from,
<code class="docutils literal notranslate"><span class="pre">number</span></code> is the number of new jobs to chain from this starting job,
and <code class="docutils literal notranslate"><span class="pre">script</span></code> is the job submission script to use (the same one you
used originally most likely).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The script can also create the initial job to start the chain.
If <code class="docutils literal notranslate"><span class="pre">jobid</span></code> is set to <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the script will first submit a
job with no dependencies and then chain the remaining <code class="docutils literal notranslate"><span class="pre">number</span></code>-1
jobs to depend on the previous.</p>
</div>
<p>You can view the job dependency using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>squeue -l -j job-id
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">job-id</span></code> is the number of the job.  A job can be canceled
using <code class="docutils literal notranslate"><span class="pre">scancel</span></code>, and the status can be checked using <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span>
<span class="pre">username</span></code>.</p>
<p>An estimate of the start time can be found via:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sqs -u username
</pre></div>
</div>
</section>
<section id="cori-gpu">
<h2>Cori GPU<a class="headerlink" href="#cori-gpu" title="Permalink to this headline">¶</a></h2>
<p>Use a SLURM script to set 1 MPI rank per GPU. In this example, we’re using 2 nodes, each with 8 GPUs.</p>
<p>Sample SLURM script <code class="docutils literal notranslate"><span class="pre">cori.MPI.CUDA.gpu.2nodes.slurm</span></code>, for this and other Cori
GPU SLURM scripts, see
<a class="reference external" href="https://github.com/AMReX-Astro/workflow/blob/main/job_scripts/cori-gpu">our Cori GPU SLURM scripts on GitHub</a></p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#</span>
<span class="linenos"> 3</span><span class="c1"># Number of nodes:</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --nodes=2</span>
<span class="linenos"> 5</span><span class="c1">#</span>
<span class="linenos"> 6</span><span class="c1">############################################</span>
<span class="linenos"> 7</span><span class="c1"># Cori GPU node configuration</span>
<span class="linenos"> 8</span><span class="c1">#</span>
<span class="linenos"> 9</span><span class="c1"># This lets SLURM pick the right number of</span>
<span class="linenos">10</span><span class="c1"># MPI tasks given the number of nodes</span>
<span class="linenos">11</span><span class="c1"># requested above. These settings will</span>
<span class="linenos">12</span><span class="c1"># assign 1 MPI task per GPU.</span>
<span class="linenos">13</span><span class="c1">#</span>
<span class="linenos">14</span><span class="c1"># (generally you won&#39;t need to change this)</span>
<span class="linenos">15</span><span class="c1">############################################</span>
<span class="linenos">16</span><span class="c1">#</span>
<span class="linenos">17</span><span class="c1"># Requests Cori GPU nodes:</span>
<span class="linenos">18</span><span class="c1">#SBATCH --constraint=gpu</span>
<span class="linenos">19</span><span class="c1">#</span>
<span class="linenos">20</span><span class="c1"># Each GPU node has 40 physical CPU cores</span>
<span class="linenos">21</span><span class="c1"># (w/ 2 hyperthreads, so 80 virtual CPU cores)</span>
<span class="linenos">22</span><span class="c1"># and 8 NVIDIA Tesla V100 GPUs.</span>
<span class="linenos">23</span><span class="c1">#</span>
<span class="linenos">24</span><span class="c1"># GPU: Assign 1 MPI tasks to each GPU</span>
<span class="linenos">25</span><span class="c1">#SBATCH --tasks-per-node=8</span>
<span class="linenos">26</span><span class="c1">#</span>
<span class="linenos">27</span><span class="c1"># We want all 8 GPUs on the node</span>
<span class="linenos">28</span><span class="c1">#SBATCH --gres=gpu:8</span>
<span class="linenos">29</span><span class="c1">#</span>
<span class="linenos">30</span><span class="c1"># Since we want the entire node (all the GPUs)</span>
<span class="linenos">31</span><span class="c1"># with 8 MPI tasks, this is 80/8 = 10 virtual CPUs</span>
<span class="linenos">32</span><span class="c1"># per MPI task.</span>
<span class="linenos">33</span><span class="c1">#SBATCH --cpus-per-task=10</span>
<span class="linenos">34</span><span class="c1">#</span>
<span class="linenos">35</span><span class="c1">#################</span>
<span class="linenos">36</span><span class="c1"># Queue &amp; Job</span>
<span class="linenos">37</span><span class="c1">#################</span>
<span class="linenos">38</span><span class="c1">#</span>
<span class="linenos">39</span><span class="c1"># Which queue to run in: debug, regular, premium, etc. ...</span>
<span class="linenos">40</span><span class="c1"># For now, don&#39;t use --qos</span>
<span class="linenos">41</span><span class="c1">## SBATCH --qos=debug</span>
<span class="linenos">42</span><span class="c1">#</span>
<span class="linenos">43</span><span class="c1"># Run for this much walltime: hh:mm:ss</span>
<span class="linenos">44</span><span class="c1">#SBATCH --time=00:15:00</span>
<span class="linenos">45</span><span class="c1">#</span>
<span class="linenos">46</span><span class="c1"># Use this job name:</span>
<span class="linenos">47</span><span class="c1">#SBATCH -J castro_gpu_job</span>
<span class="linenos">48</span><span class="c1">#</span>
<span class="linenos">49</span><span class="c1"># Send notification emails here:</span>
<span class="linenos">50</span><span class="c1">#SBATCH --mail-user=[your email address]</span>
<span class="linenos">51</span><span class="c1">#SBATCH --mail-type=ALL</span>
<span class="linenos">52</span><span class="c1">#</span>
<span class="linenos">53</span><span class="c1"># Which allocation to use:</span>
<span class="linenos">54</span><span class="c1">#SBATCH -A [your allocation ID, e.g. mABCD]</span>
<span class="linenos">55</span>
<span class="linenos">56</span><span class="c1"># On the compute node, change to the directory we submitted from</span>
<span class="linenos">57</span><span class="nb">cd</span> <span class="nv">$SLURM_SUBMIT_DIR</span>
<span class="linenos">58</span>
<span class="linenos">59</span><span class="c1"># OpenMP Configuration</span>
<span class="linenos">60</span><span class="c1"># This configuration ignores the hyperthreads</span>
<span class="linenos">61</span><span class="c1"># and assigns 1 OpenMP thread/physical core</span>
<span class="linenos">62</span><span class="nb">export</span> <span class="nv">OMP_PLACES</span><span class="o">=</span>cores
<span class="linenos">63</span><span class="nb">export</span> <span class="nv">OMP_PROC_BIND</span><span class="o">=</span><span class="nb">true</span>
<span class="linenos">64</span><span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">5</span>
<span class="linenos">65</span>
<span class="linenos">66</span>srun --cpu_bind<span class="o">=</span>cores ./Castro3d.gnu.TPROF.MPI.CUDA.ex inputs.3d.sph.testsuite
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">[your</span> <span class="pre">email</span> <span class="pre">address]</span></code> and <code class="docutils literal notranslate"><span class="pre">[your</span> <span class="pre">allocation]</span></code> with your info
(omitting the brackets).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to submit the Cori GPU SLURM script from a Cori login node.
If you submit the script from your Cori GPU interactive session, the memory
constraints you passed to <code class="docutils literal notranslate"><span class="pre">salloc</span></code> will conflict with the GPU options
specified in the SLURM script.</p>
</div>
<p>So we’ll next submit the SLURM script from a Cori login node, with the above
modules loaded:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch <span class="o">[</span>--exclusive<span class="o">]</span> cori.MPI.CUDA.gpu.2nodes.slurm
</pre></div>
</div>
<p>(The optional <code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> argument has the same meaning as for <code class="docutils literal notranslate"><span class="pre">salloc</span></code> above.)</p>
<p>We can monitor the job by checking <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">[user]</span></code> as usual with the
<code class="docutils literal notranslate"><span class="pre">cgpu</span></code> module loaded.</p>
</section>
<section id="perlmutter">
<h2>Perlmutter<a class="headerlink" href="#perlmutter" title="Permalink to this headline">¶</a></h2>
<p>Each Perlmutter node has 4 NVIDIA A100 GPUs – therefore it is best to use
4 MPI tasks per node.</p>
<p>Below is an example that launches the Sedov test compiled above with 4 GPUs per node on 4 nodes.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH -A m3018_g</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH -C gpu</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH -J Castro</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH -o sedov_%j.out</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH -t 10</span>
<span class="linenos"> 8</span><span class="c1">#SBATCH -N 4</span>
<span class="linenos"> 9</span><span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="linenos">10</span><span class="c1">#SBATCH --gpus-per-task=1</span>
<span class="linenos">11</span><span class="c1">#SBATCH --gpu-bind=map_gpu:0,1,2,3</span>
<span class="linenos">12</span>
<span class="linenos">13</span>srun -n <span class="m">16</span> ./Castro3d.gnu.TPROF.MPI.CUDA.ex inputs.3d.sph.testsuite amr.n_cell<span class="o">=</span><span class="m">256</span> <span class="m">256</span> <span class="m">256</span> amr.plot_files_output<span class="o">=</span><span class="m">0</span> amr.checkpoint_files_output<span class="o">=</span><span class="m">0</span> amr.max_grid_size<span class="o">=</span><span class="m">128</span> <span class="nv">max_step</span><span class="o">=</span><span class="m">100</span>
</pre></div>
</div>
</section>
</section>
<section id="archiving-data-to-hpss">
<h1>Archiving Data to HPSS<a class="headerlink" href="#archiving-data-to-hpss" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Access to the xfer queue is done by loading the <code class="docutils literal notranslate"><span class="pre">esslurm</span></code> queue:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load esslurm
</pre></div>
</div>
<p>Then you can use <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> and <code class="docutils literal notranslate"><span class="pre">squeue</span></code> to submit and monitor
jobs in the <code class="docutils literal notranslate"><span class="pre">xfer</span></code> queue.  Details are provided at:
<a class="reference external" href="https://docs.nersc.gov/jobs/examples/#xfer-queue">https://docs.nersc.gov/jobs/examples/#xfer-queue</a></p>
</div>
<p>The script <code class="docutils literal notranslate"><span class="pre">nersc.xfer.slurm</span></code> in
<code class="docutils literal notranslate"><span class="pre">workflow/job_scripts/cori-haswell/</span></code> can be used to archive data to
HPSS automatically. This is submitted to the xfer queue and runs the
script <code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> which continually looks for output and stores
it to HPSS.</p>
<p>To use the scripts, first create a directory in HPSS that has the same
name as the directory on lustre you are running in (just the directory
name, not the full path). E.g. if you are running in a directory call
<code class="docutils literal notranslate"><span class="pre">wdconvect/</span></code> run, then do:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hsi
mkdir wdconvect_run
</pre></div>
</div>
<p>(Note: if the <code class="docutils literal notranslate"><span class="pre">hsi</span></code> command prompts you for your password, you will need to talk to the NERSC
help desk to ask for password-less access to HPSS).</p>
<p>The script <code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> is called from the xfer job and will run in
the background and continually wait until checkpoint or plotfiles are
created (actually, it always leaves the most recent one alone, since
data may still be written to it, so it waits until there are more than
1 in the directory).  Then the script will use <code class="docutils literal notranslate"><span class="pre">htar</span></code> to archive the
plotfiles and checkpoints to HPSS. If the <code class="docutils literal notranslate"><span class="pre">htar</span></code> command was
successful, then the plotfiles are copied into a <code class="docutils literal notranslate"><span class="pre">plotfile/</span></code>
subdirectory. This is actually important, since you don’t want to try
archiving the data a second time and overwriting the stored copy,
especially if a purge took place. The same is done with checkpoint
files.</p>
<p>Additionally, if the <code class="docutils literal notranslate"><span class="pre">ftime</span></code> executable is in your path
(<code class="docutils literal notranslate"><span class="pre">ftime.f90</span></code> lives in <code class="docutils literal notranslate"><span class="pre">AMReX/Tools/Postprocessing/F_src/</span></code>), then
the script will create a file called <code class="docutils literal notranslate"><span class="pre">ftime.out</span></code> that lists the name
of the plotfile and the corresponding simulation time.</p>
<p>Finally, right when the job is submitted, the script will tar up all
of the diagnostic files, <code class="docutils literal notranslate"><span class="pre">ftime.out</span></code>, submission script, inputs and
probin, and archive them on HPSS. The .tar file is given a name that
contains the date-string to allow multiple archives to co-exist.  When
<code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> is running, it creates a lockfile (called
<code class="docutils literal notranslate"><span class="pre">process.pid</span></code>) that ensures that only one instance of the script is
running at any one time. Sometimes if the machine crashes, the
<code class="docutils literal notranslate"><span class="pre">process.pid</span></code> file will be left behind, in which case, the script
aborts. Just delete that if you know the script is not running.</p>
<p>Jobs in the xfer queue start up quickly. The best approach is to start
one as you start your main job (or make it dependent on the main
job). The sample <code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> script will wait for output and then
archive it as it is produced.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nersc-compilers.html" class="btn btn-neutral float-left" title="Compiling at NERSC" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="nersc-visualization.html" class="btn btn-neutral float-right" title="Visualization at NERSC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2021, AMReX-Astro development tem.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>