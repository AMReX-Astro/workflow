<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Managing Jobs at OLCF &mdash; AMReX-Astro 1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/documentation_options.js?v=f2a433a1"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Running Jupyter Remotely from OLCF" href="olcf-jupyter.html" />
    <link rel="prev" title="Compiling at OLCF" href="olcf-compilers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            AMReX-Astro
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">AMReX Astro basics</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="alcf.html">Working at ALCF</a></li>
<li class="toctree-l1"><a class="reference internal" href="nersc.html">Working at NERSC</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="olcf.html">Working at OLCF</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="olcf-compilers.html">Compiling at OLCF</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Managing Jobs at OLCF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#summit">Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#summit-architecture">Summit Architecture:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#requesting-allocation">Requesting Allocation:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submitting-a-job">Submitting a Job:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#writting-a-job-script">Writting a Job Script:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-a-job">Monitoring a Job:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#script-template">Script Template:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chaining-jobs">Chaining jobs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#archiving-to-hpss">Archiving to HPSS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#frontier">Frontier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#machine-details">Machine details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submitting-jobs">Submitting jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-status">Job Status</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-chaining">Job Chaining</a></li>
<li class="toctree-l4"><a class="reference internal" href="#debugging">Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="olcf-jupyter.html">Running Jupyter Remotely from OLCF</a></li>
<li class="toctree-l2"><a class="reference internal" href="olcf-andes.html">Batch Visualization on Andes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="iacs.html">Working at IACS</a></li>
<li class="toctree-l1"><a class="reference internal" href="workstations.html">Linux Workstations</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMReX-Astro</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="olcf.html">Working at OLCF</a></li>
      <li class="breadcrumb-item active">Managing Jobs at OLCF</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/olcf-workflow.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="managing-jobs-at-olcf">
<h1>Managing Jobs at OLCF<a class="headerlink" href="#managing-jobs-at-olcf" title="Link to this heading">¶</a></h1>
<section id="summit">
<h2>Summit<a class="headerlink" href="#summit" title="Link to this heading">¶</a></h2>
<section id="summit-architecture">
<h3>Summit Architecture:<a class="headerlink" href="#summit-architecture" title="Link to this heading">¶</a></h3>
<p>Let us start by reviewing the node architecture of Summit. Our goal is to provide the necessary insight to make better
decisions in the construction of our particular AMReX-Astro job scripts, and to explain how our code interacts with Summit.
All the exposed information in this section is a condensed version of the <a class="reference external" href="https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun">Summit documentation guide</a>, and should not replace it.</p>
<p>In Summit, a node is composed by two sockets: each one with 21 CPU physical cores (+1 reserved for the system), 3 GPUs and 1 RAM memory bank.
The sockets are connected by a bus allowing communication among them. Each CPU physical core may define up to 4 threads.
The whole structure of the node can be depicted as follows:</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/summit-node-description-1.png"><img alt="_images/summit-node-description-1.png" src="_images/summit-node-description-1.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Figure extracted from <code class="docutils literal notranslate"><span class="pre">https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun</span></code>.</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>A resource set is a minimal collection of CPU physical cores and GPUs, on which a certain number of MPI processes and OpenMP
threads operates through the execution of the code. Therefore, for each resource set, we need to allocate:</p>
<ul class="simple">
<li><p>A number of CPU physical cores.</p></li>
<li><p>A number of physical GPUs.</p></li>
<li><p>A number of MPI processes.</p></li>
<li><p>The number of OpenMP threads.</p></li>
</ul>
<p>where each core supports up to 4 threads; however, this option is not supported in AMReX and we will not extend our
discussion here. For now, we fix just only one thread through the whole execution of our code. The next step is to determine the maximum
number of resource sets that may fit into one node.</p>
<p>In Castro we construct each resource set with: 1 CPU physical core, 1 GPU, and only 1 MPI process.
The next step is to see how many resources sets fits into one node. According to the node architecture depicted in Figure 1,
we can fit up to 6 resource sets per node as in Figure 2.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/image56.png"><img alt="_images/image56.png" src="_images/image56.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Figure modified and extracted from <code class="docutils literal notranslate"><span class="pre">https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun</span></code>.</span><a class="headerlink" href="#id2" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="requesting-allocation">
<h3>Requesting Allocation:<a class="headerlink" href="#requesting-allocation" title="Link to this heading">¶</a></h3>
<p>To allocate the resource sets we need to summon the command <code class="docutils literal notranslate"><span class="pre">bsub</span></code> in addition to some flags:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-nnodes</span></code></p></td>
<td><p>allocates the number of nodes we need to run our code. Is important to perform the calculation
described in the previous section to select the correct number of nodes in our setup.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-W</span></code></p></td>
<td><p>allocates the walltime of the selected nodes. The format we use in Summit is [hours:]minutes, there is
no room for seconds in Summit. The maximum walltime that we can request is 03:00 (three hours).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-alloc_flags</span></code></p></td>
<td><p>allocates the maximum number of threads available per CPU core. By default the option is <code class="docutils literal notranslate"><span class="pre">smt4</span></code> that
allows 4 threads per core. However, since we will consider only one thread through the whole execution
we will setup the option <code class="docutils literal notranslate"><span class="pre">smt1</span></code>. Also <code class="docutils literal notranslate"><span class="pre">-alloc_flags</span></code> stands for more options, however we are only
interested in the one discussed before.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-J</span></code></p></td>
<td><p>defines the name of the allocation. The value <code class="docutils literal notranslate"><span class="pre">%J</span></code> correspond to the allocation ID number.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-o</span></code></p></td>
<td><p>defines the <strong>output name that contains the standard output stream</strong>, after running all the jobs inside the requested
allocation.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-e</span></code></p></td>
<td><p>defines the <strong>output file name containing the standard error stream</strong>, similar to the <code class="docutils literal notranslate"><span class="pre">-o</span></code> flag. If <code class="docutils literal notranslate"><span class="pre">-e</span></code> is not supplied, then
the <code class="docutils literal notranslate"><span class="pre">-o</span></code> option is assumed by default.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-q</span></code></p></td>
<td><p>defines the queue on which our application will run. There are several options, however, we alternate
between two options: the standard production queue <code class="docutils literal notranslate"><span class="pre">batch</span></code> and the debugging queue <code class="docutils literal notranslate"><span class="pre">debug</span></code>. The <code class="docutils literal notranslate"><span class="pre">debug</span></code> queue is designed
to allocate an small number of nodes in order to see that our code is running smoothly without bugs.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-Is</span></code></p></td>
<td><p>flags for interactive job followed by the shell name. The Unix bash shell option is <code class="docutils literal notranslate"><span class="pre">/bin/bash</span></code>. This flag is very useful
for debugging, because the standard output can be checked as the code is running. Is important to mention that any interactive
job can only be summoned by command line and not by running a bash script.</p></td>
</tr>
</tbody>
</table>
<p>For example, if we want to allocate one node to run an interactive job in the debug queue for 30 minutes we may setup:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1">bsub<span class="w"> </span>-nnodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>-q<span class="w"> </span>debug<span class="w"> </span>-W<span class="w"> </span><span class="m">0</span>:30<span class="w"> </span>-P<span class="w"> </span>ast106<span class="w"> </span>-alloc_flags<span class="w"> </span>smt1<span class="w"> </span>-J<span class="w"> </span>example<span class="w"> </span>-o<span class="w"> </span>stdout_to_show.%J<span class="w"> </span>-e<span class="w"> </span>stderr_to_show.%J<span class="w"> </span>-Is<span class="w"> </span>/bin/bash</span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>An interactive job can only be allocated by the use of the command line. No script can be defined for interactive jobs.</p>
</div>
</section>
<section id="submitting-a-job">
<h3>Submitting a Job:<a class="headerlink" href="#submitting-a-job" title="Link to this heading">¶</a></h3>
<p>Once our allocation is granted, is important to load the same modules used in the compilation process of the executable and
export the variable <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> to setup the number of threads per MPI process.</p>
<p>In Castro, we have used the following modules:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.2.0
module<span class="w"> </span>load<span class="w"> </span>cuda/11.5.2
module<span class="w"> </span>load<span class="w"> </span>python
</pre></div>
</div>
<p>and fixed only one thread per MPI process by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>The next step is to submit our job. The command <cite>jsrun</cite>, provided with the <em>total number of resource sets</em>, the
<em>number of CPU physical cores per resource set</em>, <em>the number of GPUs per resource set</em>, <em>the number of MPI processes allocated per resource set</em>,
works as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">jsrun<span class="w"> </span>-n<span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>resource<span class="w"> </span>sets<span class="o">]</span><span class="w"> </span>-c<span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>CPU<span class="w"> </span>physical<span class="w"> </span>cores<span class="o">]</span><span class="w"> </span>-g<span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>GPUs<span class="o">]</span><span class="w"> </span>-a<span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>MPI<span class="w"> </span>processes<span class="o">]</span><span class="w"> </span>-r<span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>max<span class="w"> </span>resources<span class="w"> </span>per<span class="w"> </span>node<span class="o">]</span><span class="w"> </span>./<span class="o">[</span>executable<span class="o">]</span><span class="w"> </span><span class="o">[</span>executable<span class="w"> </span>inputs<span class="o">]</span></span>
</pre></div></div><p>In Castro we will use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">jsrun<span class="w"> </span>-n<span class="w"> </span><span class="o">[</span>number<span class="w"> </span>of<span class="w"> </span>resource<span class="w"> </span>sets<span class="o">]</span><span class="w"> </span>-a1<span class="w"> </span>-c1<span class="w"> </span>-g1<span class="w"> </span>-r6<span class="w"> </span>./<span class="nv">$CASTRO</span><span class="w"> </span><span class="nv">$INPUTS</span></span>
</pre></div></div><p>where the <code class="docutils literal notranslate"><span class="pre">CASTRO</span></code> and <code class="docutils literal notranslate"><span class="pre">INPUTS</span></code> environment variables are placeholders to the executable and input file names respectively.</p>
<p>Now, in order to use all the resources we have allocated to run our jobs, the number of resource sets should match the number of AMReX boxes (grids)
of the corresponding level with the biggest number of them. Let us consider an extract piece from a Castro problem standard output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>INITIAL<span class="w"> </span>GRIDS
Level<span class="w"> </span><span class="m">0</span><span class="w">   </span><span class="m">2</span><span class="w"> </span>grids<span class="w">  </span><span class="m">32768</span><span class="w"> </span>cells<span class="w">  </span><span class="m">100</span><span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">128</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">128</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span>
Level<span class="w"> </span><span class="m">1</span><span class="w">   </span><span class="m">8</span><span class="w"> </span>grids<span class="w">  </span><span class="m">131072</span><span class="w"> </span>cells<span class="w">  </span><span class="m">100</span><span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">128</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">128</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span>
Level<span class="w"> </span><span class="m">2</span><span class="w">   </span><span class="m">8</span><span class="w"> </span>grids<span class="w">  </span><span class="m">524288</span><span class="w"> </span>cells<span class="w">  </span><span class="m">100</span><span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span>
Level<span class="w"> </span><span class="m">3</span><span class="w">   </span><span class="m">32</span><span class="w"> </span>grids<span class="w">  </span><span class="m">2097152</span><span class="w"> </span>cells<span class="w">  </span><span class="m">100</span><span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span>
Level<span class="w"> </span><span class="m">4</span><span class="w">   </span><span class="m">128</span><span class="w"> </span>grids<span class="w">  </span><span class="m">7864320</span><span class="w"> </span>cells<span class="w">  </span><span class="m">93</span>.75<span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span>
Level<span class="w"> </span><span class="m">5</span><span class="w">   </span><span class="m">480</span><span class="w"> </span>grids<span class="w">  </span><span class="m">30408704</span><span class="w"> </span>cells<span class="w">  </span><span class="m">90</span>.625<span class="w"> </span>%<span class="w"> </span>of<span class="w"> </span>domain
<span class="w">          </span>smallest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">128</span><span class="w">  </span>biggest<span class="w"> </span>grid:<span class="w"> </span><span class="m">256</span><span class="w"> </span>x<span class="w"> </span><span class="m">256</span>
</pre></div>
</div>
<p>In this example, Level 5 contains the biggest number of AMReX boxes: 480. From here, we may assert that a good allocation for this problem are
480 resource sets, equivalent to 80 nodes by setting 6 resources per node. However, note that that Level 0 uses only 2 AMReX boxes, this implies that
from the 480 resources available, 398 resources will remain idle until the two working processes sweep the entire Level 0.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Therefore, is important, if possible, to keep the number of boxes
on each level balanced to maximize the use of the allocated resources.</p>
</div>
</section>
<section id="writting-a-job-script">
<h3>Writting a Job Script:<a class="headerlink" href="#writting-a-job-script" title="Link to this heading">¶</a></h3>
<p>In order to make our life easier, instead of submitting an allocation
command line, loading the modules, setting the threads/MPI process,
and writing another command line to submit our jobs, we can make an
script to pack all these command into one executable <code class="docutils literal notranslate"><span class="pre">.sh</span></code> file,
that can be submitted via <code class="docutils literal notranslate"><span class="pre">bsub</span></code> just once.</p>
<p>We start our job script, summoning the shell with the statement
<code class="docutils literal notranslate"><span class="pre">!/bin/bash</span></code>. Then we add the <code class="docutils literal notranslate"><span class="pre">bsub</span></code> allocations flags, starting
with <code class="docutils literal notranslate"><span class="pre">#BSUB</span></code> as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#BSUB -P ast106</span>
<span class="c1">#BSUB -W 2:00</span>
<span class="c1">#BSUB -nnodes 80</span>
<span class="c1">#BSUB -alloc_flags smt1</span>
<span class="c1">#BSUB -J luna_script</span>
<span class="c1">#BSUB -o luna_output.%J</span>
<span class="c1">#BSUB -e luna_sniffing_output.%J</span>
</pre></div>
</div>
<p>In addition we add the modules statements, fixing only one thread per MPI process:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.2.0
module<span class="w"> </span>load<span class="w"> </span>cuda/11.5.2
module<span class="w"> </span>load<span class="w"> </span>python

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>and define the environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CASTRO</span><span class="o">=</span>./Castro2d.gnu.MPI.CUDA.ex
<span class="nv">INPUTS</span><span class="o">=</span>inputs_luna

<span class="nv">n_res</span><span class="o">=</span><span class="m">480</span><span class="w">                </span><span class="c1"># The max allocated number of resource sets is</span>
<span class="nv">n_cpu_cores_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">    </span><span class="c1"># nnodes * n_max_res_per_node. In this case we will</span>
<span class="nv">n_mpi_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">          </span><span class="c1"># use all the allocated resource sets to run the job</span>
<span class="nv">n_gpu_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">          </span><span class="c1"># below.</span>
<span class="nv">n_max_res_per_node</span><span class="o">=</span><span class="m">6</span>
</pre></div>
</div>
<p>Once the allocation ends, the job is downgraded/killed, leaving us as we started. As we pointed out, the maximum allocation
time in Summit is 03:00 (three hours), but, we may need sometimes weeks, months, or maybe years to complete
our runs. Now is when the automatic restarting section of the script comes to our salvation.</p>
<p>From here we can add an optional (or mandatory) setting to our script. As the code executes,
after a certain number of timesteps, the code creates checkpoint files of the form <code class="docutils literal notranslate"><span class="pre">chkxxxxxxx</span></code>, <code class="docutils literal notranslate"><span class="pre">chkxxxxxx</span></code>
or <code class="docutils literal notranslate"><span class="pre">chkxxxxx</span></code>. This checkpoint files can be read by our executable and run from the simulation time where
the checkpoint was created. This is implemented as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span>find_chk_file<span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="c1"># find_chk_file takes a single argument -- the wildcard pattern</span>
<span class="w">   </span><span class="c1"># for checkpoint files to look through</span>
<span class="w">   </span><span class="nv">chk</span><span class="o">=</span><span class="nv">$1</span>

<span class="w">   </span><span class="c1"># find the latest 2 restart files.  This way if the latest didn&#39;t</span>
<span class="w">   </span><span class="c1"># complete we fall back to the previous one.</span>
<span class="w">   </span><span class="nv">temp_files</span><span class="o">=</span><span class="k">$(</span>find<span class="w"> </span>.<span class="w"> </span>-maxdepth<span class="w"> </span><span class="m">1</span><span class="w"> </span>-name<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">chk</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>-print<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span><span class="p">|</span><span class="w"> </span>tail<span class="w"> </span>-2<span class="k">)</span>
<span class="w">   </span><span class="nv">restartFile</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span>f<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="si">${</span><span class="nv">temp_files</span><span class="si">}</span>
<span class="w">   </span><span class="k">do</span>
<span class="w">      </span><span class="c1"># the Header is the last thing written -- if it&#39;s there, update the restart file</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-f<span class="w"> </span><span class="si">${</span><span class="nv">f</span><span class="si">}</span>/Header<span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">         </span><span class="nv">restartFile</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="w">      </span><span class="k">fi</span>
<span class="w">   </span><span class="k">done</span>
<span class="o">}</span>

<span class="c1"># look for 7-digit chk files</span>
find_chk_file<span class="w"> </span><span class="s2">&quot;*chk???????&quot;</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">   </span><span class="c1"># look for 6-digit chk files</span>
<span class="w">   </span>find_chk_file<span class="w"> </span><span class="s2">&quot;*chk??????&quot;</span>
<span class="k">fi</span>

<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">   </span><span class="c1"># look for 5-digit chk files</span>
<span class="w">   </span>find_chk_file<span class="w"> </span><span class="s2">&quot;*chk?????&quot;</span>
<span class="k">fi</span>

<span class="c1"># restartString will be empty if no chk files are found -- i.e. new run</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">   </span><span class="nv">restartString</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="k">else</span>
<span class="w">   </span><span class="nv">restartString</span><span class="o">=</span><span class="s2">&quot;amr.restart=</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="k">fi</span>
</pre></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">find_chk_file</span></code> searches the submission directory for
checkpoint files.  Because AMReX appends digits as the number of steps
increase (with a minimum of 5 digits), we search for files with
7-digits, 6-digits, and then finally 5-digits, to ensure we pick up
the latest file.</p>
<p>We can also ask the job manager to send a warning signal some amount
of time before the allocation expires by passing <code class="docutils literal notranslate"><span class="pre">-wa</span> <span class="pre">'signal'</span></code> and
<code class="docutils literal notranslate"><span class="pre">-wt</span> <span class="pre">'[hour:]minute'</span></code> to <code class="docutils literal notranslate"><span class="pre">bsub</span></code>.  We can then have bash create a
<code class="docutils literal notranslate"><span class="pre">dump_and_stop</span></code> file when it receives the signal, which will tell
Castro to output a checkpoint file and exit cleanly after it finishes
the current timestep.  An important detail that I couldn’t find
documented anywhere is that the job manager sends the signal to all
the processes in the job, not just the submission script, and we have
to use a signal that is ignored by default so Castro doesn’t
immediately crash upon receiving it.  SIGCHLD, SIGURG, and SIGWINCH
are the only signals that fit this requirement and of these, SIGURG is
the least likely to be triggered by other events.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#BSUB -wa URG</span>
<span class="c1">#BSUB -wt 2</span>

...

<span class="k">function</span><span class="w"> </span>sig_handler<span class="w"> </span><span class="o">{</span>
<span class="w">   </span>touch<span class="w"> </span>dump_and_stop
<span class="w">   </span><span class="c1"># disable this signal handler</span>
<span class="w">   </span><span class="nb">trap</span><span class="w"> </span>-<span class="w"> </span>URG
<span class="w">   </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;BATCH: allocation ending soon; telling Castro to dump a checkpoint and stop&quot;</span>
<span class="o">}</span>
<span class="nb">trap</span><span class="w"> </span>sig_handler<span class="w"> </span>URG
</pre></div>
</div>
<p>We use the <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> command to launch Castro on the compute nodes. In
order for bash to handle the warning signal before Castro exits, we
must put <code class="docutils literal notranslate"><span class="pre">jsrun</span></code> in the background and use the shell builtin
<code class="docutils literal notranslate"><span class="pre">wait</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jsrun<span class="w"> </span>-n<span class="nv">$n_res</span><span class="w"> </span>-c<span class="nv">$n_cpu_cores_per_res</span><span class="w"> </span>-a<span class="nv">$n_mpi_per_res</span><span class="w"> </span>-g<span class="nv">$n_gpu_per_res</span><span class="w"> </span>-r<span class="nv">$n_max_res_per_node</span><span class="w"> </span><span class="nv">$CASTRO</span><span class="w"> </span><span class="nv">$INPUTS</span><span class="w"> </span><span class="si">${</span><span class="nv">restartString</span><span class="si">}</span><span class="w"> </span><span class="p">&amp;</span>
<span class="nb">wait</span>
<span class="c1"># use jswait to wait for Castro (job step 1/1) to finish and get the exit code</span>
jswait<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>Finally, once the script is completed and saved as <code class="docutils literal notranslate"><span class="pre">luna_script.sh</span></code>, we can submit it by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">bsub<span class="w"> </span>luna_script.sh</span>
</pre></div></div></section>
<section id="monitoring-a-job">
<h3>Monitoring a Job:<a class="headerlink" href="#monitoring-a-job" title="Link to this heading">¶</a></h3>
<p>You can monitor the status of your jobs using <code class="docutils literal notranslate"><span class="pre">bjobs</span></code>. Also, a slightly nicer view of your jobs can be viewed using <code class="docutils literal notranslate"><span class="pre">jobstat</span></code> as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">jobstat<span class="w"> </span>-u<span class="w"> </span>username</span>
</pre></div></div></section>
<section id="script-template">
<h3>Script Template:<a class="headerlink" href="#script-template" title="Link to this heading">¶</a></h3>
<p>Packing all the information before, lead us to the following script template</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#BSUB -P ast106</span>
<span class="linenos"> 3</span><span class="c1">#BSUB -W 2:00</span>
<span class="linenos"> 4</span><span class="c1">#BSUB -nnodes 80</span>
<span class="linenos"> 5</span><span class="c1">#BSUB -alloc_flags smt1</span>
<span class="linenos"> 6</span><span class="c1">#BSUB -J luna_script</span>
<span class="linenos"> 7</span><span class="c1">#BSUB -o luna_output.%J</span>
<span class="linenos"> 8</span><span class="c1">#BSUB -e luna_sniffing_output.%J</span>
<span class="linenos"> 9</span><span class="c1">#BSUB -wa URG</span>
<span class="linenos">10</span><span class="c1">#BSUB -wt 2</span>
<span class="linenos">11</span>
<span class="linenos">12</span>module<span class="w"> </span>load<span class="w"> </span>gcc/10.2.0
<span class="linenos">13</span>module<span class="w"> </span>load<span class="w"> </span>cuda/11.5.2
<span class="linenos">14</span>module<span class="w"> </span>load<span class="w"> </span>python
<span class="linenos">15</span>
<span class="linenos">16</span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="nv">CASTRO</span><span class="o">=</span>./Castro2d.gnu.MPI.CUDA.ex
<span class="linenos">19</span><span class="nv">INPUTS</span><span class="o">=</span>inputs_luna
<span class="linenos">20</span>
<span class="linenos">21</span><span class="nv">n_res</span><span class="o">=</span><span class="m">480</span><span class="w">                </span><span class="c1"># The max allocated number of resource sets is</span>
<span class="linenos">22</span><span class="nv">n_cpu_cores_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">    </span><span class="c1"># nnodes * n_max_res_per_node. In this case we will</span>
<span class="linenos">23</span><span class="nv">n_mpi_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">          </span><span class="c1"># use all the allocated resource sets to run the job below,</span>
<span class="linenos">24</span><span class="nv">n_gpu_per_res</span><span class="o">=</span><span class="m">1</span><span class="w">          </span><span class="c1"># however we can define more enviroment variables to allocate two jobs</span>
<span class="linenos">25</span><span class="nv">n_max_res_per_node</span><span class="o">=</span><span class="m">6</span><span class="w">     </span><span class="c1"># simultaneous jobs, where n_res = n_res_1 + n_res2 allocates for two jobs.</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="k">function</span><span class="w"> </span>find_chk_file<span class="w"> </span><span class="o">{</span>
<span class="linenos">28</span><span class="w">    </span><span class="c1"># find_chk_file takes a single argument -- the wildcard pattern</span>
<span class="linenos">29</span><span class="w">    </span><span class="c1"># for checkpoint files to look through</span>
<span class="linenos">30</span><span class="w">    </span><span class="nv">chk</span><span class="o">=</span><span class="nv">$1</span>
<span class="linenos">31</span>
<span class="linenos">32</span><span class="w">    </span><span class="c1"># find the latest 2 restart files.  This way if the latest didn&#39;t</span>
<span class="linenos">33</span><span class="w">    </span><span class="c1"># complete we fall back to the previous one.</span>
<span class="linenos">34</span><span class="w">    </span><span class="nv">temp_files</span><span class="o">=</span><span class="k">$(</span>find<span class="w"> </span>.<span class="w"> </span>-maxdepth<span class="w"> </span><span class="m">1</span><span class="w"> </span>-name<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">chk</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>-print<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span><span class="p">|</span><span class="w"> </span>tail<span class="w"> </span>-2<span class="k">)</span>
<span class="linenos">35</span><span class="w">    </span><span class="nv">restartFile</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="linenos">36</span><span class="w">    </span><span class="k">for</span><span class="w"> </span>f<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="si">${</span><span class="nv">temp_files</span><span class="si">}</span>
<span class="linenos">37</span><span class="w">    </span><span class="k">do</span>
<span class="linenos">38</span><span class="w">        </span><span class="c1"># the Header is the last thing written -- if it&#39;s there, update the restart file</span>
<span class="linenos">39</span><span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-f<span class="w"> </span><span class="si">${</span><span class="nv">f</span><span class="si">}</span>/Header<span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="linenos">40</span><span class="w">            </span><span class="c1"># The scratch FS sometimes gives I/O errors when trying to read</span>
<span class="linenos">41</span><span class="w">            </span><span class="c1"># from recently-created files, which crashes Castro. Avoid this by</span>
<span class="linenos">42</span><span class="w">            </span><span class="c1"># making sure we can read from all the data files.</span>
<span class="linenos">43</span><span class="w">            </span><span class="k">if</span><span class="w"> </span>head<span class="w"> </span>--quiet<span class="w"> </span>-c1<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">f</span><span class="si">}</span><span class="s2">/Header&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">f</span><span class="si">}</span><span class="s2">&quot;</span>/Level_*/*<span class="w"> </span>&gt;/dev/null<span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="linenos">44</span><span class="w">                </span><span class="nv">restartFile</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">f</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="linenos">45</span><span class="w">            </span><span class="k">fi</span>
<span class="linenos">46</span><span class="w">        </span><span class="k">fi</span>
<span class="linenos">47</span><span class="w">    </span><span class="k">done</span>
<span class="linenos">48</span><span class="o">}</span>
<span class="linenos">49</span>
<span class="linenos">50</span><span class="c1"># look for 7-digit chk files</span>
<span class="linenos">51</span>find_chk_file<span class="w"> </span><span class="s2">&quot;*chk???????&quot;</span>
<span class="linenos">52</span>
<span class="linenos">53</span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="linenos">54</span><span class="w">    </span><span class="c1"># look for 6-digit chk files</span>
<span class="linenos">55</span><span class="w">    </span>find_chk_file<span class="w"> </span><span class="s2">&quot;*chk??????&quot;</span>
<span class="linenos">56</span><span class="k">fi</span>
<span class="linenos">57</span>
<span class="linenos">58</span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="linenos">59</span><span class="w">    </span><span class="c1"># look for 5-digit chk files</span>
<span class="linenos">60</span><span class="w">    </span>find_chk_file<span class="w"> </span><span class="s2">&quot;*chk?????&quot;</span>
<span class="linenos">61</span><span class="k">fi</span>
<span class="linenos">62</span>
<span class="linenos">63</span><span class="c1"># restartString will be empty if no chk files are found -- i.e. new run</span>
<span class="linenos">64</span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="linenos">65</span><span class="w">    </span><span class="nv">restartString</span><span class="o">=</span><span class="s2">&quot;&quot;</span>
<span class="linenos">66</span><span class="k">else</span>
<span class="linenos">67</span><span class="w">    </span><span class="nv">restartString</span><span class="o">=</span><span class="s2">&quot;amr.restart=</span><span class="si">${</span><span class="nv">restartFile</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="linenos">68</span><span class="k">fi</span>
<span class="linenos">69</span>
<span class="linenos">70</span><span class="c1"># clean up any run management files left over from previous runs</span>
<span class="linenos">71</span>rm<span class="w"> </span>-f<span class="w"> </span>dump_and_stop
<span class="linenos">72</span>
<span class="linenos">73</span><span class="nv">warning_time</span><span class="o">=</span><span class="k">$(</span>bjobs<span class="w"> </span>-noheader<span class="w"> </span>-o<span class="w"> </span>action_warning_time<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$LSB_JOBID</span><span class="s2">&quot;</span><span class="k">)</span>
<span class="linenos">74</span><span class="c1"># The `-wa URG -wt &lt;n&gt;` options tell bsub to send SIGURG to all processes n</span>
<span class="linenos">75</span><span class="c1"># minutes before the runtime limit, so we can exit gracefully.</span>
<span class="linenos">76</span><span class="c1"># SIGURG is ignored by default, so it won&#39;t make Castro crash.</span>
<span class="linenos">77</span><span class="k">function</span><span class="w"> </span>sig_handler<span class="w"> </span><span class="o">{</span>
<span class="linenos">78</span><span class="w">    </span>touch<span class="w"> </span>dump_and_stop
<span class="linenos">79</span><span class="w">    </span><span class="c1"># disable this signal handler</span>
<span class="linenos">80</span><span class="w">    </span><span class="nb">trap</span><span class="w"> </span>-<span class="w"> </span>URG
<span class="linenos">81</span><span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;BATCH: </span><span class="nv">$warning_time</span><span class="s2"> left in allocation; telling Castro to dump a checkpoint and stop&quot;</span>
<span class="linenos">82</span><span class="o">}</span>
<span class="linenos">83</span><span class="nb">trap</span><span class="w"> </span>sig_handler<span class="w"> </span>URG
<span class="linenos">84</span>
<span class="linenos">85</span><span class="c1"># execute jsrun in the background then use the builtin wait so the shell can</span>
<span class="linenos">86</span><span class="c1"># handle the signal</span>
<span class="linenos">87</span>jsrun<span class="w"> </span>-n<span class="nv">$n_res</span><span class="w"> </span>-c<span class="nv">$n_cpu_cores_per_res</span><span class="w"> </span>-a<span class="nv">$n_mpi_per_res</span><span class="w"> </span>-g<span class="nv">$n_gpu_per_res</span><span class="w"> </span>-r<span class="nv">$n_max_res_per_node</span><span class="w"> </span><span class="nv">$CASTRO</span><span class="w"> </span><span class="nv">$INPUTS</span><span class="w"> </span><span class="si">${</span><span class="nv">restartString</span><span class="si">}</span><span class="w"> </span><span class="p">&amp;</span>
<span class="linenos">88</span><span class="nb">wait</span>
<span class="linenos">89</span><span class="c1"># use jswait to wait for Castro (job step 1/1) to finish and get the exit code</span>
<span class="linenos">90</span>jswait<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="chaining-jobs">
<h3>Chaining jobs<a class="headerlink" href="#chaining-jobs" title="Link to this heading">¶</a></h3>
<p>The script <code class="docutils literal notranslate"><span class="pre">job_scripts/summit/chain_submit.sh</span></code> can be used to setup job dependencies,
i.e., a job chain.</p>
<p>First you submit a job as usual using <code class="docutils literal notranslate"><span class="pre">bsub</span></code>, and make note of the
job-id that it prints upon submission (the same id you would see with
<code class="docutils literal notranslate"><span class="pre">bjobs</span></code> or <code class="docutils literal notranslate"><span class="pre">jobstat</span></code>).  Then you setup N jobs to depend on the one
you just submitted as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">chain_submit.sh<span class="w"> </span>job-id<span class="w"> </span>N<span class="w"> </span>submit_script.sh</span>
</pre></div></div><p>where you replace <code class="docutils literal notranslate"><span class="pre">job-id</span></code> with the id return from your first
submission, replace <code class="docutils literal notranslate"><span class="pre">N</span></code> with the number of additional jobs, and
replace <code class="docutils literal notranslate"><span class="pre">submit_script</span></code> with the name of the script you use to
submit the job.  This will queue up N additional jobs, each depending
on the previous.  Your submission script should use the automatic
restarting features discussed above.</p>
</section>
</section>
<section id="archiving-to-hpss">
<h2>Archiving to HPSS<a class="headerlink" href="#archiving-to-hpss" title="Link to this heading">¶</a></h2>
<p>You can access HPSS from submit using the data transfer nodes by submitting a job
via SLURM:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>-N<span class="w"> </span><span class="m">1</span><span class="w"> </span>-t<span class="w"> </span><span class="m">15</span>:00<span class="w"> </span>-A<span class="w"> </span>ast106<span class="w"> </span>--cluster<span class="w"> </span>dtn<span class="w"> </span>test_hpss.sh</span>
</pre></div></div><p>where <code class="docutils literal notranslate"><span class="pre">test_hpss.sh</span></code> is a SLURM script that contains the <code class="docutils literal notranslate"><span class="pre">htar</span></code>
commands needed to archive your data.  This uses <code class="docutils literal notranslate"><span class="pre">slurm</span></code> as the job
manager.</p>
<p>An example is provided by the <code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> archiving script and
associated <code class="docutils literal notranslate"><span class="pre">summit_hpss.submit</span></code> submission script in
<code class="docutils literal notranslate"><span class="pre">jobs_scripts/summit/</span></code>.  Together these will detect new plotfiles as
they are generated, tar them up (using <code class="docutils literal notranslate"><span class="pre">htar</span></code>) and archive them onto
HPSS.  They will also store the inputs, probin, and other runtime
generated files.  If <code class="docutils literal notranslate"><span class="pre">ftime</span></code> is found in your path, it will also
create a file called <code class="docutils literal notranslate"><span class="pre">ftime.out</span></code> that lists the simulation time
corresponding to each plotfile.</p>
<p>Once the plotfiles are archived they are moved to a subdirectory under
your run directory called <code class="docutils literal notranslate"><span class="pre">plotfiles/</span></code>.</p>
<p>To use this, we do the following:</p>
<ol class="arabic">
<li><p>Enter the HPSS system via <code class="docutils literal notranslate"><span class="pre">hsi</span></code></p></li>
<li><p>Create the output directory – this should have the same name as the directory
you are running in on summit</p></li>
<li><p>Exit HPSS</p></li>
<li><p>Launch the script via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>summit_hpss.submit</span>
</pre></div></div><p>It will for the full time you asked, searching for plotfiles as
they are created and moving them to HPSS as they are produced (it
will always leave the very last plotfile alone, since it can’t tell
if it is still being written).</p>
</li>
</ol>
<p>Files may be unarchived in bulk from HPSS on OLCF systems using the
<code class="docutils literal notranslate"><span class="pre">hpss_xfer.py</span></code> script, which is available in the job_scripts
directory. It requires Python 3 to be loaded to run. The command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">./hpss_xfer.py<span class="w"> </span>plt00000<span class="w"> </span>-s<span class="w"> </span>hpss_dir<span class="w"> </span>-o<span class="w"> </span>plotfile_dir</span>
</pre></div></div><p>will fetch <code class="docutils literal notranslate"><span class="pre">hpss_dir/plt00000.tar</span></code> from the HPSS filesystem and
unpack it in <code class="docutils literal notranslate"><span class="pre">plotfile_dir</span></code>. If run with no arguments in the problem
launch directory, the script will attempt to recover all plotfiles
archived by <code class="docutils literal notranslate"><span class="pre">process.titan</span></code>. Try running <code class="code docutils literal notranslate"><span class="pre">./hpss_xfer.py</span> <span class="pre">--help</span></code>
for a description of usage and arguments.</p>
</section>
<section id="frontier">
<h2>Frontier<a class="headerlink" href="#frontier" title="Link to this heading">¶</a></h2>
<section id="machine-details">
<h3>Machine details<a class="headerlink" href="#machine-details" title="Link to this heading">¶</a></h3>
<p>Queue policies are here:
<a class="reference external" href="https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#scheduling-policy">https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#scheduling-policy</a></p>
<p>Filesystem is called <code class="docutils literal notranslate"><span class="pre">orion</span></code>, and is Lustre:
<a class="reference external" href="https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-and-storage">https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-and-storage</a></p>
</section>
<section id="submitting-jobs">
<h3>Submitting jobs<a class="headerlink" href="#submitting-jobs" title="Link to this heading">¶</a></h3>
<p>Frontier uses SLURM.</p>
<p>Here’s a script that runs with 2 nodes using all 8 GPUs per node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -A AST106</span>
<span class="c1">#SBATCH -J testing</span>
<span class="c1">#SBATCH -o %x-%j.out</span>
<span class="c1">#SBATCH -t 00:05:00</span>
<span class="c1">#SBATCH -p batch</span>
<span class="c1"># here N is the number of compute nodes</span>
<span class="c1">#SBATCH -N 2</span>
<span class="c1">#SBATCH --ntasks-per-node=8</span>
<span class="c1">#SBATCH --cpus-per-task=7</span>
<span class="c1">#SBATCH --gpus-per-task=1</span>
<span class="c1">#SBATCH --gpu-bind=closest</span>

<span class="nv">EXEC</span><span class="o">=</span>Castro3d.hip.x86-trento.MPI.HIP.ex
<span class="nv">INPUTS</span><span class="o">=</span>inputs.3d.sph

module<span class="w"> </span>load<span class="w"> </span>PrgEnv-gnu<span class="w"> </span>craype-accel-amd-gfx90a<span class="w"> </span>cray-mpich<span class="w"> </span>rocm/5.3.0
module<span class="w"> </span>load<span class="w"> </span>amd-mixed/5.3.0

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NMPI_PER_NODE</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TOTAL_NMPI</span><span class="o">=</span><span class="k">$((</span><span class="w"> </span><span class="si">${</span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="si">}</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="si">${</span><span class="nv">NMPI_PER_NODE</span><span class="si">}</span><span class="w"> </span><span class="k">))</span>

srun<span class="w"> </span>-n<span class="si">${</span><span class="nv">TOTAL_NMPI</span><span class="si">}</span><span class="w"> </span>-N<span class="si">${</span><span class="nv">SLURM_JOB_NUM_NODES</span><span class="si">}</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span><span class="w"> </span>--gpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span>./<span class="nv">$EXEC</span><span class="w"> </span><span class="nv">$INPUTS</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As of June 2023, it is necessary to explicitly use <code class="docutils literal notranslate"><span class="pre">-n</span></code> and <code class="docutils literal notranslate"><span class="pre">-N</span></code> on the <code class="docutils literal notranslate"><span class="pre">srun</span></code> line.</p>
</div>
<p>The job is submitted as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>frontier.slurm</span>
</pre></div></div><p>where <code class="docutils literal notranslate"><span class="pre">frontier.slurm</span></code> is the name of the submission script.</p>
<p>A sample job script that includes the automatic restart functions can be found here:
<a class="reference external" href="https://github.com/AMReX-Astro/workflow/blob/main/job_scripts/frontier/frontier.slurm">https://github.com/AMReX-Astro/workflow/blob/main/job_scripts/frontier/frontier.slurm</a></p>
<p>Also see the WarpX docs: <a class="reference external" href="https://warpx.readthedocs.io/en/latest/install/hpc/frontier.html">https://warpx.readthedocs.io/en/latest/install/hpc/frontier.html</a></p>
</section>
<section id="job-status">
<h3>Job Status<a class="headerlink" href="#job-status" title="Link to this heading">¶</a></h3>
<p>You can check on the status of your jobs via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">squeue<span class="w"> </span>--me</span>
</pre></div></div><p>and get an estimated start time via:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">squeue<span class="w"> </span>--me<span class="w"> </span>--start</span>
</pre></div></div></section>
<section id="job-chaining">
<h3>Job Chaining<a class="headerlink" href="#job-chaining" title="Link to this heading">¶</a></h3>
<p>The script <a class="reference external" href="https://github.com/AMReX-Astro/workflow/blob/main/job_scripts/slurm/chainslurm.sh">chainslurm.sh</a> can be used to start
a job chain, with each job depending on the previous.  For example, to start up
10 jobs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">chainslurm<span class="w"> </span>-1<span class="w"> </span><span class="m">10</span><span class="w"> </span>frontier.slurm</span>
</pre></div></div><p>If you want to add the chain to an existing queued job, change the <code class="docutils literal notranslate"><span class="pre">-1</span></code> to the job-id
of the existing job.</p>
</section>
<section id="debugging">
<h3>Debugging<a class="headerlink" href="#debugging" title="Link to this heading">¶</a></h3>
<p>For debugging:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">rocgdb<span class="w"> </span>--args<span class="w"> </span>./Castro2d.hip.x86-trento.MPI.HIP.ex<span class="w"> </span>inputs_2d.testsuite</span>
</pre></div></div><p>then do <code class="docutils literal notranslate"><span class="pre">run</span></code> at the debugger prompt.</p>
</section>
<section id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading">¶</a></h3>
<p>Workaround to prevent hangs for collectives:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FI_MR_CACHE_MONITOR</span><span class="o">=</span>memhooks
</pre></div>
</div>
<p>Some AMReX reports are that it hangs if the initial Arena size is too big, and we should do</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>amrex.the_arena_init_size<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p>The arena size would then grow as needed with time.  There is a suggestion that if the size is
larger than</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="olcf-compilers.html" class="btn btn-neutral float-left" title="Compiling at OLCF" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="olcf-jupyter.html" class="btn btn-neutral float-right" title="Running Jupyter Remotely from OLCF" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2021, AMReX-Astro development tem.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>