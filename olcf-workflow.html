<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Managing Jobs at OLCF &mdash; AMReX-Astro 1.0 documentation</title>
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Running Jupyter Remotely from OLCF" href="olcf-jupyter.html" />
    <link rel="prev" title="Compiling at OLCF" href="olcf-compilers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> AMReX-Astro
          </a>
              <div class="version">
                1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">AMReX Astro basics</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nersc.html">Working at NERSC</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="olcf.html">Working at OLCF</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="olcf-compilers.html">Compiling at OLCF</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Managing Jobs at OLCF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#summit">Summit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submission-scripts">Submission scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submitting-and-monitoring">Submitting and monitoring</a></li>
<li class="toctree-l4"><a class="reference internal" href="#automatic-restarting">Automatic restarting</a></li>
<li class="toctree-l4"><a class="reference internal" href="#chaining-jobs">Chaining jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#archiving-to-hpss">Archiving to HPSS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="olcf-jupyter.html">Running Jupyter Remotely from OLCF</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="iacs.html">Working at IACS</a></li>
<li class="toctree-l1"><a class="reference internal" href="workstations.html">Linux Workstations</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AMReX-Astro</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="olcf.html">Working at OLCF</a> &raquo;</li>
      <li>Managing Jobs at OLCF</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/olcf-workflow.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="managing-jobs-at-olcf">
<h1>Managing Jobs at OLCF<a class="headerlink" href="#managing-jobs-at-olcf" title="Permalink to this headline">¶</a></h1>
<section id="summit">
<h2>Summit<a class="headerlink" href="#summit" title="Permalink to this headline">¶</a></h2>
<section id="submission-scripts">
<h3>Submission scripts<a class="headerlink" href="#submission-scripts" title="Permalink to this headline">¶</a></h3>
<p>On Summit, we have a few different examples of PBS batch
scripts. <code class="docutils literal notranslate"><span class="pre">run_amrex_gpu_tutorials.summit</span></code> is a shallow copy of the
<a class="reference external" href="https://github.com/AMReX-Codes/amrex/blob/development/Tutorials/GPU/run.summit">AMReX tutorial script</a>,
and is more verbose about what different flags and options will do.</p>
<p>The Castro GPU batch script example is <code class="docutils literal notranslate"><span class="pre">summit_16nodes.sh</span></code>, more job
script examples for Castro can be found <a class="reference external" href="https://github.com/AMReX-Astro/Castro/tree/master/Util/scaling/sedov/summit_201905">here</a>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#BSUB -P projectid</span>
<span class="linenos"> 3</span><span class="c1">#BSUB -W 50</span>
<span class="linenos"> 4</span><span class="c1">#BSUB -nnodes 16</span>
<span class="linenos"> 5</span><span class="c1">#BSUB -alloc_flags smt1</span>
<span class="linenos"> 6</span><span class="c1">#BSUB -J Sedov_gpu</span>
<span class="linenos"> 7</span><span class="c1">#BSUB -o Sedov_gpu.%J</span>
<span class="linenos"> 8</span><span class="c1">#BSUB -e Sedov_gpu.%J</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="nb">cd</span> <span class="nv">$LS_SUBCWD</span>
<span class="linenos">11</span>
<span class="linenos">12</span>module load gcc/10.2.0
<span class="linenos">13</span>module load cuda/11.2.0
<span class="linenos">14</span>
<span class="linenos">15</span><span class="nv">inputs_file</span><span class="o">=</span>inputs.3d.sph
<span class="linenos">16</span>
<span class="linenos">17</span><span class="nv">n_mpi</span><span class="o">=</span><span class="m">96</span> <span class="c1"># 16 nodes * 6 gpu per node</span>
<span class="linenos">18</span><span class="nv">n_omp</span><span class="o">=</span><span class="m">1</span>
<span class="linenos">19</span><span class="nv">n_gpu</span><span class="o">=</span><span class="m">1</span>
<span class="linenos">20</span><span class="nv">n_cores</span><span class="o">=</span><span class="m">1</span>
<span class="linenos">21</span><span class="nv">n_rs_per_node</span><span class="o">=</span><span class="m">6</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$n_omp</span>
<span class="linenos">24</span>
<span class="linenos">25</span><span class="nv">Castro_ex</span><span class="o">=</span>./Castro3d.pgi.MPI.CUDA.ex
<span class="linenos">26</span>
<span class="linenos">27</span>jsrun -n <span class="nv">$n_mpi</span> -r <span class="nv">$n_rs_per_node</span> -c <span class="nv">$n_cores</span> -a <span class="m">1</span> -g <span class="nv">$n_gpu</span> <span class="nv">$Castro_ex</span> <span class="nv">$inputs_file</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You should explicitly include the “module loads” for the GCC and CUDA version you are
using in the submission script, otherwise your job may not run.  In the example above,
we load <code class="docutils literal notranslate"><span class="pre">gcc/10.2.0</span></code> and <code class="docutils literal notranslate"><span class="pre">cuda/11.2.0</span></code>.</p>
</div>
<p>The Nyx example shows running MPI+CUDA, MPI+CUDA with one mpi process
nvvp output, and MPI+OMP
<code class="docutils literal notranslate"><span class="pre">run_3_tests_same_node.summit</span></code>. <code class="docutils literal notranslate"><span class="pre">run_template.summit</span></code> gives
example syntax for jsrun:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">### Begin BSUB Options</span>
<span class="linenos"> 3</span><span class="c1">#BSUB -P &lt;OLCF_PROJECT_ID&gt;</span>
<span class="linenos"> 4</span><span class="c1">#BSUB -J &lt;JOB_NAME&gt;</span>
<span class="linenos"> 5</span><span class="c1">#BSUB -W &lt;HH:MM&gt;</span>
<span class="linenos"> 6</span><span class="c1">#BSUB -nnodes &lt;NUMBER_OF_NODES&gt;</span>
<span class="linenos"> 7</span><span class="c1">#BSUB -alloc_flags &quot;smt4&quot;</span>
<span class="linenos"> 8</span><span class="c1">### End BSUB Options and begin shell commands</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># =====================</span>
<span class="linenos">11</span><span class="c1"># BSUB parameters</span>
<span class="linenos">12</span><span class="c1"># -nnodes = number of nodes</span>
<span class="linenos">13</span>
<span class="hll"><span class="linenos">14</span><span class="c1"># JSRUN parameters</span>
</span><span class="hll"><span class="linenos">15</span><span class="c1"># -n:--nrs          number of resource sets</span>
</span><span class="hll"><span class="linenos">16</span><span class="c1">#   (alternatively,</span>
</span><span class="hll"><span class="linenos">17</span><span class="c1"># -r:rs_per_host     number of resource sets per node)</span>
</span><span class="hll"><span class="linenos">18</span><span class="c1"># -a:tasks_per_rs    number of MPI tasks/ranks per resource set</span>
</span><span class="hll"><span class="linenos">19</span><span class="c1"># -c:cpu_per_rs      number of CPU cores per resource set</span>
</span><span class="hll"><span class="linenos">20</span><span class="c1"># -g:gpu_per_rs      number of GPUs per resource set</span>
</span><span class="linenos">21</span>
<span class="linenos">22</span><span class="c1"># Summit: Each node has:</span>
<span class="linenos">23</span><span class="c1">#         2  Sockets</span>
<span class="linenos">24</span><span class="c1">#         3  GPUs per socket (6 total)</span>
<span class="linenos">25</span><span class="c1">#         21 CPUs per socket (42 total)</span>
<span class="linenos">26</span><span class="c1">#         4  Hardware Threads per CPU (168 total)</span>
<span class="linenos">27</span><span class="c1">#</span>
<span class="linenos">28</span><span class="c1"># On Summit: AMReX recommended initial configuration:</span>
<span class="linenos">29</span><span class="c1">#             one GPU per MPI rank,</span>
<span class="linenos">30</span><span class="c1">#             one resource set per socket.</span>
<span class="linenos">31</span><span class="c1">#             (-r2 -c21 -g3 -a3)</span>
<span class="linenos">32</span><span class="c1"># =====================</span>
<span class="linenos">33</span>
<span class="hll"><span class="linenos">34</span><span class="c1">#MPI + GPU version</span>
</span><span class="hll"><span class="linenos">35</span><span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
</span><span class="hll"><span class="linenos">36</span>jsrun --nrs <span class="m">6</span> --tasks_per_rs <span class="m">1</span> --cpu_per_rs <span class="m">7</span> --gpu_per_rs <span class="m">1</span> --rs_per_host <span class="m">6</span> --latency_priority CPU-CPU --launch_distribution packed --bind packed:7 js_task_info <span class="p">|</span> sort
</span><span class="hll"><span class="linenos">37</span><span class="c1">#jsrun -n 6 -a 1 -g 1 -c 7 --bind=packed:1</span>
</span><span class="hll"><span class="linenos">38</span>
</span><span class="hll"><span class="linenos">39</span><span class="c1">#MPI + OpenMP version</span>
</span><span class="hll"><span class="linenos">40</span><span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">7</span>
</span><span class="hll"><span class="linenos">41</span>jsrun --nrs <span class="m">6</span> --tasks_per_rs <span class="m">1</span> --cpu_per_rs <span class="m">7</span> --rs_per_host <span class="m">6</span> --latency_priority CPU-CPU --launch_distribution packed --bind packed:7 js_task_info <span class="p">|</span> sort
</span><span class="hll"><span class="linenos">42</span><span class="c1">#jsrun -n 6 -a 1 -g 1 -c 7 --bind=packed:7</span>
</span></pre></div>
</div>
<p>This can be visualized using <a class="reference external" href="https://jsrunvisualizer.olcf.ornl.gov/index.html">https://jsrunvisualizer.olcf.ornl.gov/index.html</a></p>
<span id="fig-gpu-threads"></span><table class="docutils align-default" id="id1">
<caption><span class="caption-text">Comparison of jsrun process assignment for MPI + OpenMP and MPI + GPU work distribution.</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="_images/jsrunVisualizer-MPI+OMP.png"><img alt="a" src="_images/jsrunVisualizer-MPI+OMP.png" style="width: 100%;" /></a></p></td>
<td><p><a class="reference internal" href="_images/jsrunVisualizer-MPI+GPU.png"><img alt="b" src="_images/jsrunVisualizer-MPI+GPU.png" style="width: 100%;" /></a></p></td>
</tr>
<tr class="row-even"><td><div class="line-block">
<div class="line">MPI + OpenMP</div>
</div>
</td>
<td><div class="line-block">
<div class="line">MPI + GPU</div>
</div>
</td>
</tr>
</tbody>
</table>
<p>The example script directory is: <a class="reference external" href="https://github.com/AMReX-Astro/workflow/tree/master/job_scripts/summit">https://github.com/AMReX-Astro/workflow/tree/master/job_scripts/summit</a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We are defaulting to one hardware thread per CPU, since this is the configuration suggested by OLCF</p>
</div>
</section>
<section id="submitting-and-monitoring">
<h3>Submitting and monitoring<a class="headerlink" href="#submitting-and-monitoring" title="Permalink to this headline">¶</a></h3>
<p>Jobs are submitted using the <code class="docutils literal notranslate"><span class="pre">bsub</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bsub script.sh
</pre></div>
</div>
<p>You can monitor the status of your jobs using <code class="docutils literal notranslate"><span class="pre">bjobs</span></code>.</p>
<p>A slightly nicer view of your jobs can be viewed using <code class="docutils literal notranslate"><span class="pre">jobstat</span></code> as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>jobstat -u username
</pre></div>
</div>
</section>
<section id="automatic-restarting">
<h3>Automatic restarting<a class="headerlink" href="#automatic-restarting" title="Permalink to this headline">¶</a></h3>
<p>Often we run a single simulation over many queue submissions with each
starting from the latest checkpoint file.  The script
<code class="docutils literal notranslate"><span class="pre">job_scripts/summit/submit_restart.sh</span></code> shows how to automatically
detect the last checkpoint file and restart from it.  This allows you
to submit your jobs without any manual intervention.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">find_chk_file</span></code> searches the submission directory for
checkpoint files.  Because AMReX appends digits as the number of steps
increase (with a minimum of 5 digits), we search for files with
7-digits, 6-digits, and then finally 5-digits, to ensure we pick up
the latest file.</p>
</section>
<section id="chaining-jobs">
<h3>Chaining jobs<a class="headerlink" href="#chaining-jobs" title="Permalink to this headline">¶</a></h3>
<p>The script <code class="docutils literal notranslate"><span class="pre">job_scripts/summit/chain_submit.sh</span></code> can be used to setup job dependencies,
i.e., a job chain.</p>
<p>First you submit a job as usual using <code class="docutils literal notranslate"><span class="pre">bsub</span></code>, and make note of the
job-id that it prints upon submission (the same id you would see with
<code class="docutils literal notranslate"><span class="pre">bjobs</span></code> or <code class="docutils literal notranslate"><span class="pre">jobstat</span></code>).  Then you setup N jobs to depend on the one
you just submitted as:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chain_submit.sh job-id N submit_script.sh
</pre></div>
</div>
<p>where you replace <code class="docutils literal notranslate"><span class="pre">job-id</span></code> with the id return from your first
submission, replace <code class="docutils literal notranslate"><span class="pre">N</span></code> with the number of additional jobs, and
replace <code class="docutils literal notranslate"><span class="pre">submit_script</span></code> with the name of the script you use to
submit the job.  This will queue up N additional jobs, each depending
on the previous.  Your submission script should use the automatic
restarting features discussed above.</p>
</section>
<section id="archiving-to-hpss">
<h3>Archiving to HPSS<a class="headerlink" href="#archiving-to-hpss" title="Permalink to this headline">¶</a></h3>
<p>You can access HPSS from submit using the data transfer nodes by submitting a job
via SLURM:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch -N <span class="m">1</span> -t <span class="m">15</span>:00 -A ast106 --cluster dtn test_hpss.sh
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">test_hpss.sh</span></code> is a SLURM script that contains the <code class="docutils literal notranslate"><span class="pre">htar</span></code>
commands needed to archive your data.  This uses <code class="docutils literal notranslate"><span class="pre">slurm</span></code> as the job
manager.</p>
<p>An example is provided by the <code class="docutils literal notranslate"><span class="pre">process.xrb</span></code> archiving script and
associated <code class="docutils literal notranslate"><span class="pre">summit_hpss.submit</span></code> submission script in
<code class="docutils literal notranslate"><span class="pre">jobs_scripts/summit/</span></code>.  Together these will detect new plotfiles as
they are generated, tar them up (using <code class="docutils literal notranslate"><span class="pre">htar</span></code>) and archive them onto
HPSS.  They will also store the inputs, probin, and other runtime
generated files.  If <code class="docutils literal notranslate"><span class="pre">ftime</span></code> is found in your path, it will also
create a file called <code class="docutils literal notranslate"><span class="pre">ftime.out</span></code> that lists the simulation time
corresponding to each plotfile.</p>
<p>Once the plotfiles are archived they are moved to a subdirectory under
your run directory called <code class="docutils literal notranslate"><span class="pre">plotfiles/</span></code>.</p>
<p>To use this, we do the following:</p>
<ol class="arabic">
<li><p>Enter the HPSS system via <code class="docutils literal notranslate"><span class="pre">hsi</span></code></p></li>
<li><p>Create the output directory – this should have the same name as the directory
you are running in on summit</p></li>
<li><p>Exit HPSS</p></li>
<li><p>Launch the script via:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch summit_hpss.submit
</pre></div>
</div>
<p>It will for the full time you asked, searching for plotfiles as
they are created and moving them to HPSS as they are produced (it
will always leave the very last plotfile alone, since it can’t tell
if it is still being written).</p>
</li>
</ol>
<p>Files may be unarchived in bulk from HPSS on OLCF systems using the
<code class="docutils literal notranslate"><span class="pre">hpss_xfer.py</span></code> script, which is available in the job_scripts
directory. It requires Python 3 to be loaded to run. The command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./hpss_xfer.py plt00000 -s hpss_dir -o plotfile_dir
</pre></div>
</div>
<p>will fetch <code class="docutils literal notranslate"><span class="pre">hpss_dir/plt00000.tar</span></code> from the HPSS filesystem and
unpack it in <code class="docutils literal notranslate"><span class="pre">plotfile_dir</span></code>. If run with no arguments in the problem
launch directory, the script will attempt to recover all plotfiles
archived by <code class="docutils literal notranslate"><span class="pre">process.titan</span></code>. Try running <code class="code docutils literal notranslate"><span class="pre">./hpss_xfer.py</span> <span class="pre">--help</span></code>
for a description of usage and arguments.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="olcf-compilers.html" class="btn btn-neutral float-left" title="Compiling at OLCF" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="olcf-jupyter.html" class="btn btn-neutral float-right" title="Running Jupyter Remotely from OLCF" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, AMReX-Astro development tem.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>